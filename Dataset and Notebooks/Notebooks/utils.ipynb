{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV88v66a3aYZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.inception import InceptionOutputs\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzfbbPVRfHNY"
   },
   "outputs": [],
   "source": [
    "def data_sampling(indices):\n",
    "  return torch.utils.data.sampler.SubsetRandomSampler(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSFeMnrKeyo2"
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def data_preprocess(data_path, sample_ratio, batch_size):\n",
    "  # Create data transforms\n",
    "  data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "  # Get dataset from folder and apply data transforms\n",
    "  dataset = datasets.ImageFolder(root = \"{}data\".format(data_path), transform = data_transforms)\n",
    "    \n",
    "  # Get a sample of the data randomly\n",
    "  num_samples = int(len(dataset) * sample_ratio)\n",
    "  indices = np.random.choice(range(len(dataset)), num_samples, replace = False)\n",
    "\n",
    "  # Split the data into training, test, and validation sets\n",
    "  train_size = int(0.7 * num_samples)\n",
    "  test_size = int(0.2 * num_samples)\n",
    "  val_size = num_samples - train_size - test_size\n",
    "\n",
    "  train_indices = indices[ : train_size]\n",
    "  test_indices = indices[train_size : train_size + test_size]\n",
    "  val_indices = indices[train_size + test_size : ]\n",
    "\n",
    "  samples = [data_sampling(i) for i in [train_indices, test_indices, val_indices]]\n",
    "\n",
    "  # Create data loaders for training, test, and validation sets\n",
    "  train_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[0], num_workers = 4, pin_memory = True)\n",
    "  test_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[1], num_workers = 4, pin_memory = True)\n",
    "  val_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[2], num_workers = 4, pin_memory = True)\n",
    "\n",
    "  return dataset, train_loader, train_indices, test_loader, test_indices, val_loader, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gIfwoRhe1zF_"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, data_size, dtype, criterion, data_path, model_name):\n",
    "  _loss, _pred, _true, _accuracy = 0.0, [], [], []\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      _loss += loss.item() * inputs.size(0)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      _pred.extend(predicted.cpu().numpy())\n",
    "      _true.extend(labels.cpu().numpy())\n",
    "\n",
    "  _loss /= len(data_size)\n",
    "  _accuracy = accuracy_score(_true, _pred)  \n",
    "  _recall = recall_score(_true, _pred, average='macro')\n",
    "  _precision = precision_score(_true, _pred, average='macro')\n",
    "  _fscore = f1_score(_true, _pred, average='macro')\n",
    "\n",
    "  print('{}: Accuracy: {:.4f} | Loss: {:.4f} | Recall: {:.4f} | Precision: {:.4f} | F-score: {:.4f}'.format(dtype, _accuracy, _loss, _recall, _precision, _fscore))\n",
    "  print(\"\")\n",
    "\n",
    "  if(dtype == \"TEST\"):\n",
    "    cm = confusion_matrix(_true, _pred)\n",
    "    plt.figure(figsize = (20, 20))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = dataset.classes)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "\n",
    "  else:\n",
    "    return _accuracy, _loss\n",
    "  \"\"\"  \n",
    "    plt.imshow(cm, cmap = plt.cm.Blues)\n",
    "    plt.title(\"{}_{}SET_CONFUSION_MATRIX\".format(model_name, dtype))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"{}_{}SET_CONFUSION_MATRIX.png\".format(model_name, dtype))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPfNnVOIUbXw"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, model_name, num_epochs):\n",
    "  losses, accuracies, true, pred, v_accuracies, v_losses = [], [], [], [], [], []\n",
    "  for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "      for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, dim = 1)\n",
    "        loss = criterion(outputs.logits if isinstance(outputs, InceptionOutputs) else outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        train_accuracy += torch.sum(preds == labels.data)\n",
    "        pred.extend(preds.cpu().numpy())\n",
    "        true.extend(labels.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({'Accuracy': train_accuracy.item()/len(train_indices), 'Loss': train_loss/len(train_indices), 'Precision': precision_score(true, pred, average='macro'), 'Recall': recall_score(true, pred, average='macro'), 'F1 Score': f1_score(true, pred, average = 'macro')})\n",
    "        pbar.update()      \n",
    "    \n",
    "    val_accuracy, val_loss = evaluate_model(model, val_loader, val_indices, 'VALIDATION', criterion, data_path, \"ResNet18\")\n",
    "\n",
    "    v_accuracies.append(val_accuracy)\n",
    "    v_losses.append(val_loss)\n",
    "    losses.append(train_loss/len(train_indices))\n",
    "    accuracies.append(train_accuracy.item()/len(train_indices))\n",
    "  print(losses, v_losses)\n",
    "  save_metrics(losses, accuracies, model_name)\n",
    "  return losses, accuracies, v_accuracies, v_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pn4QipdVwfF6"
   },
   "outputs": [],
   "source": [
    "def plot_TSNE(train_loader, device, model):\n",
    "  #Obtain the TSNE Plot for the data\n",
    "  features = []\n",
    "  labels = []\n",
    "  for images, targets in train_loader:\n",
    "      images = images.to(device)\n",
    "      targets = targets.to(device)\n",
    "      with torch.no_grad():\n",
    "          output = model(images)\n",
    "          features.append(output.cpu().numpy())\n",
    "          labels.append(targets.cpu().numpy())\n",
    "\n",
    "  features = np.vstack(features)\n",
    "  labels = np.concatenate(labels)\n",
    "\n",
    "  tsne = TSNE(n_components=2, perplexity = 25, learning_rate = 600, n_iter = 900)\n",
    "  tsne_features = tsne.fit_transform(features)\n",
    "\n",
    "  tsne_df = pd.DataFrame(data=tsne_features, columns=['t-SNE 1', 't-SNE 2'])\n",
    "  tsne_df['label'] = labels\n",
    "\n",
    "  # Plot the t-SNE plot with seaborn\n",
    "  sns.scatterplot(data=tsne_df, x='t-SNE 1', y='t-SNE 2', hue='label', palette='tab10')\n",
    "  plt.title('t-SNE Plot')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1V58AuLxZB7"
   },
   "outputs": [],
   "source": [
    "def plot_within_class_variance(dataset):\n",
    "  #Get the class labels and the number of classes\n",
    "  class_labels = dataset.classes\n",
    "  num_classes = len(class_labels)\n",
    "\n",
    "  #Get the number of images per class\n",
    "  num_images_per_class = []\n",
    "  for i in range(num_classes):\n",
    "      class_indices = np.where(np.array(dataset.targets) == i)[0]\n",
    "      num_images_per_class.append(len(class_indices))\n",
    "\n",
    "  #Compute the mean and variance of the images per class\n",
    "  mean_num_images = np.mean(num_images_per_class)\n",
    "  var_num_images = np.var(num_images_per_class)\n",
    "\n",
    "  #Plot the within-class variance\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.bar(class_labels, num_images_per_class)\n",
    "  ax.axhline(y=mean_num_images, linestyle='--', color='r', label='Mean')\n",
    "  ax.axhspan(mean_num_images - np.sqrt(var_num_images), mean_num_images + np.sqrt(var_num_images),\n",
    "            alpha=0.2, color='y', label='Variance')\n",
    "  ax.legend()\n",
    "  plt.xticks(rotation = 0)\n",
    "  plt.ylabel('Number of Images')\n",
    "  plt.xlabel('Classes')\n",
    "  plt.title('Within-Class Variance Plot')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwHXAaTTyMk8"
   },
   "outputs": [],
   "source": [
    "def plot_model_curves(losses, accuracies, v_accuracies, v_losses):\n",
    "  #Plotting the Loss and Accuracy Curves\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "  ax1.plot(losses, label = \"Training Loss\")\n",
    "  ax1.plot(v_losses, label = \"Validation Loss\")\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_title('Training and Validation Loss Curve')\n",
    "  ax1.legend()\n",
    "\n",
    "  ax2.plot(accuracies, label = \"Training Accuracy\")\n",
    "  ax2.plot(v_accuracies, label = \"Validation Accuracy\")\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_title('Training and Validation Accuracy Curve')\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1I8gXbIBqS0xeQEk_zxOfR7n5AFB67LjF",
     "timestamp": 1678846566393
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
